[Ch10. Neural Network - ReLU, 초기값 정하기]

NN for XOR
	input, hidden, output layer

너무 많은 Layer에선 Backpropagation을 통한 weight 설정이 제대로 되지 않음
	Why?
		- Layer중에서 * 곱에 대한 backpropagation을 하면 미분값이 0~1이므로 (Sigmoid라서)
	  	최종적 값이 0에 가까운 작은 값이 되게 될 것이다
	  ==>> Vanishing gradient 문제
	
	Hinton "non-linear"이 좀 잘 못 됐다
		=> "Sigmoid 말고 ReLU (Rectified Linear Unit) 쓰자!"
				 [ __/ 모양 ]


@@ Problem 1. non-linear한 잘못된 타입 사용했다

ReLU (Rectified Linear Unit)
	L1 = tf.nn.relu(tf.matmul(X, W1) + b1
		.
		.
		.
	hypothesis = tf.sigmoid(L10, W11) + b11 
		* 마지막단은 계속 sigmoid 사용! 0~1의 값이 나와야 하기 때문에

	=> accuracy 1로 증가!

	Leaky ReLU, Maxout, ELU, tanh 등등 개발됨



@@ Problem 2. 초기값을 잘 못 설정했다

초기값을 0으로 주었을 때
	- 다른 입력값의 gradient가 다 0이 되어버림
	=> 절대 초기값이 0이면 안된다!

Hinton (2006) - Restricted Boatman Machine(RBM)을 사용한 Deep Belief Net (현재 잘 사용 x)
	[Forward할 때 input한 x], [Backward로 나온 x] 의 차이를 보며 weight 조절
		= Encord, Decord	(Label 없이도 구할 수 있음)
	
	=> 이 과정을 각 2개의 layer마다 진행 = 초기화 값
	= Fine Tunning

	=> RBM 그래도 약간 복잡, 다른 방법이 있다!

Xavier initialization (2010)
	입력 개수, 출력 개수에 따라 초기값을 설정하면 된다!
	(fan_in) (fan_out)
	
	2010
	  W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)
	
	2015
	  W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in/2)

아직 연구되고 있는 분야들이다


@@ Problem 3. 컴퓨터가 너무 느렸다

@@ Problem 4. 라벨링된 데이터셋이 너무 작았다



Overfitting
	tratining data 	-> 99% Acc
	   test data	-> 85% Acc
	
	layer의 개수가 많아질 수록 test data의 accuracy가 떨어진다면 overfitting 된 것이다
	=> Regularization
		= l2leg

Dropout (2014)
	= randomly set some neurons to zero in the forward pass"
	어떻게 가능?
		학습시 몇 feature가 쉰다

	dropout_rate = tf.placeholder("float")
	_L1 = tf.nn.relu(tf.add(tf.matmul(X, W1), B1))
	L1 = tf.nn.dropout(_L1, dropout_rate)

	* 학습때는 0.7정도로, 하지만 실제 Evaluation 때는 dropout_rate = 1

Ensemble
	[Learning Model #1] [Learning Model #2] ... [Learning Model #1000]
		|	|	|	|	|	|	|
		 -----------------------|-----------------------

				     Combiner	

					|

				Ensemble Prediction
	=> 2~5% 성능 향상 가능


Neural Network 만들기 (=레고 조립과 같다)

	Feedforward neural network
		이때까지 배운 layer - layer - layer -> ^Y 방법

	Fast forward
		중간 layer를 넘어선 layer에 값이 전달되는 방법

	Split & merge
		layer들이 나눠졌다가 다시 합쳐지는 방법

	Recurrent network
		옆으로 나아가는 방법












